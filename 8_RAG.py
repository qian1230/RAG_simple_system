import os
import ssl
import sys
import time
import uuid
import re
import socket
from typing import List, Dict, Optional, Awaitable
from urllib.parse import urlparse
from dotenv import load_dotenv

# ---------------------- åŸºç¡€ç¯å¢ƒé…ç½® ----------------------
load_dotenv()
if sys.platform == "win32":
    ssl._create_default_https_context = ssl._create_unverified_context

# ---------------------- ç¬¬ä¸‰æ–¹ä¾èµ–å¯¼å…¥ ----------------------
from qdrant_client import QdrantClient
from qdrant_client.http.models import (
    VectorParams, PointStruct, Filter, FieldCondition, MatchValue
)
from llama_index.embeddings.dashscope import DashScopeEmbedding
from llama_index.core.embeddings import BaseEmbedding
from llama_index.core.bridge.pydantic import PrivateAttr

# ---------------------- æ™ºèƒ½åŠ©æ‰‹æ ¸å¿ƒå¯¼å…¥ ----------------------
from hello_agents import SimpleAgent, HelloAgentsLLM, ToolRegistry
from hello_agents.tools.builtin.rag_tool import RAGTool

# ---------------------- å…¨å±€åµŒå…¥æ¨¡å‹å®ä¾‹ ----------------------
_global_embedder = None


# ---------------------- ä¿®å¤ï¼šå®Œæ•´çš„å®‰å…¨åµŒå…¥æ¨¡å‹åŒ…è£…ç±» ----------------------
class SafeDashScopeEmbedding(BaseEmbedding):
    """å®‰å…¨çš„DashScopeåµŒå…¥æ¨¡å‹åŒ…è£…ç±»ï¼Œå®Œå…¨å…¼å®¹llama-indexè§„èŒƒ"""
    _embedder: DashScopeEmbedding = PrivateAttr()

    def __init__(
            self,
            model_name: str = "text-embedding-v1",
            api_key: str = None,
            timeout: int = 30,
    ):
        super().__init__()
        if not api_key:
            raise ValueError("API key must be provided for DashScopeEmbedding")

        # åˆå§‹åŒ–åŸç”ŸDashScopeåµŒå…¥æ¨¡å‹
        self._embedder = DashScopeEmbedding(
            model_name=model_name,
            api_key=api_key,
            timeout=timeout
        )

    def _get_query_embedding(self, query: str) -> List[float]:
        """ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼ˆåŒæ­¥ç‰ˆï¼Œç¬¦åˆBaseEmbeddingè§„èŒƒï¼‰"""
        try:
            vec = self._embedder.get_text_embedding(query.strip() or "ç©ºæ–‡æœ¬")
            # æ ¼å¼æ ‡å‡†åŒ–
            if isinstance(vec, list):
                vec_norm = [float(x) for x in vec]
            elif hasattr(vec, "tolist"):
                vec_norm = vec.tolist()
                vec_norm = [float(x) for x in vec_norm]
            else:
                raise ValueError(f"å‘é‡æ ¼å¼é”™è¯¯ï¼š{type(vec)}")

            # ç»´åº¦æ ¡éªŒï¼ˆå›ºå®š1536ç»´ï¼‰
            if len(vec_norm) != 1536:
                print(f"âš ï¸ å‘é‡ç»´åº¦å¼‚å¸¸ï¼šæœŸæœ›1536ï¼Œå®é™…{len(vec_norm)}ï¼Œè‡ªåŠ¨ä¿®æ­£")
                if len(vec_norm) > 1536:
                    vec_norm = vec_norm[:1536]
                else:
                    vec_norm += [0.0] * (1536 - len(vec_norm))

            # æ£€æŸ¥æ˜¯å¦å…¨é›¶
            if all(v == 0.0 for v in vec_norm):
                print(f"âš ï¸ æŸ¥è¯¢åµŒå…¥è¿”å›å…¨é›¶å‘é‡ï¼š{query[:50]}...")

            return vec_norm
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢åµŒå…¥å¤±è´¥ï¼š{query[:50]}... é”™è¯¯ï¼š{str(e)[:100]}")
            return [0.0] * 1536

    async def _aget_query_embedding(self, query: str) -> List[float]:
        """ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼ˆå¼‚æ­¥ç‰ˆï¼Œå¿…é¡»å®ç°çš„æŠ½è±¡æ–¹æ³•ï¼‰"""
        # åŒæ­¥è½¬å¼‚æ­¥
        return self._get_query_embedding(query)

    def _get_text_embedding(self, text: str) -> List[float]:
        """ç”Ÿæˆæ–‡æœ¬å‘é‡ï¼ˆåŒæ­¥ç‰ˆï¼Œç¬¦åˆBaseEmbeddingè§„èŒƒï¼‰"""
        return self._get_query_embedding(text)

    async def _aget_text_embedding(self, text: str) -> List[float]:
        """ç”Ÿæˆæ–‡æœ¬å‘é‡ï¼ˆå¼‚æ­¥ç‰ˆï¼Œå¿…é¡»å®ç°çš„æŠ½è±¡æ–¹æ³•ï¼‰"""
        return self._get_text_embedding(text)

    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡ç”Ÿæˆæ–‡æœ¬å‘é‡ï¼ˆåŒæ­¥ç‰ˆï¼Œæ ¸å¿ƒä¿®å¤ï¼‰"""
        valid_vectors = []
        for text in texts:
            valid_vectors.append(self._get_text_embedding(text))
        return valid_vectors

    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡ç”Ÿæˆæ–‡æœ¬å‘é‡ï¼ˆå¼‚æ­¥ç‰ˆï¼Œå¿…é¡»å®ç°çš„æŠ½è±¡æ–¹æ³•ï¼‰"""
        return self._get_text_embeddings(texts)

    # å…¼å®¹æ—§ä»£ç çš„æ–¹æ³•
    def encode(self, texts: List[str]) -> List[List[float]]:
        """å…¼å®¹æ—§ä»£ç çš„æ‰¹é‡åµŒå…¥æ–¹æ³•"""
        return self._get_text_embeddings(texts)

    def get_text_embedding(self, text: str) -> List[float]:
        """å…¼å®¹åŸç”ŸDashScopeEmbeddingçš„æ–¹æ³•"""
        return self._get_text_embedding(text)


# ---------------------- è¾…åŠ©å‡½æ•°ï¼šåŸºç¡€å·¥å…· ----------------------
def _enhanced_pdf_processing(path: str) -> str:
    """å¢å¼ºPDFå¤„ç†ï¼ˆå®Œæ•´ç‰ˆï¼Œæ”¯æŒå¤šé¡µPDFè§£æ+æ ¼å¼æ¸…ç†ï¼‰"""
    try:
        import fitz  # PyMuPDF
        doc = fitz.open(path)
        text = ""
        page_count = len(doc)
        # éå†æ‰€æœ‰é¡µé¢ï¼Œä¿ç•™é¡µç ä¿¡æ¯
        for page_num, page in enumerate(doc, 1):
            page_text = page.get_text()
            # æ¸…ç†PDFæ–‡æœ¬ä¸­çš„å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œ
            page_text = page_text.replace("\n\n", "\n").strip()
            if page_text:
                text += f"=== ç¬¬{page_num}é¡µ ===\n{page_text}\n\n"
        doc.close()
        print(f"[RAG] PDFè§£ææˆåŠŸ: {path}ï¼Œå…±{page_count}é¡µï¼Œæå–æ–‡æœ¬{len(text)}å­—ç¬¦")
        return text
    except ImportError:
        print("[ERROR] PyMuPDFæœªå®‰è£…ï¼è¯·æ‰§è¡Œï¼špip install pymupdf")
        return ""
    except Exception as e:
        print(f"[WARNING] PDFå¢å¼ºå¤„ç†å¤±è´¥ {path}: {str(e)[:100]}")
        return _fallback_text_reader(path)


def _get_markitdown_instance():
    """æ¨¡æ‹ŸMarkItDownå®ä¾‹ï¼ˆå…¼å®¹éPDFæ–‡ä»¶ï¼‰"""

    class MockMarkItDown:
        def convert(self, path):
            with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                return type('obj', (object,), {"text_content": f.read()})

    return MockMarkItDown()


def _fallback_text_reader(path: str) -> str:
    """é™çº§æ–‡æœ¬è¯»å–å™¨"""
    try:
        with open(path, 'r', encoding='utf-8', errors='ignore') as f:
            return f.read()
    except Exception as e:
        print(f"[WARNING] é™çº§è¯»å–å¤±è´¥ {path}: {e}")
        return ""


def get_text_embedder():
    """è·å–ç»Ÿä¸€åµŒå…¥æ¨¡å‹ï¼ˆåŸºäºDashScopeï¼‰- ä¿®å¤æ ¸å¿ƒé—®é¢˜"""
    global _global_embedder
    if _global_embedder is not None:
        return _global_embedder

    dashscope_api_key = os.getenv("DASHSCOPE_API_KEY")
    if not dashscope_api_key:
        raise ValueError("âŒ DASHSCOPE_API_KEY æœªé…ç½®ï¼")

    # åˆå§‹åŒ–ä¿®å¤åçš„å®‰å…¨åµŒå…¥æ¨¡å‹
    embedder = SafeDashScopeEmbedding(
        model_name="text-embedding-v1",
        api_key=dashscope_api_key,
        timeout=30
    )

    _global_embedder = embedder
    return embedder


def embed_query(query: str) -> List[float]:
    """å•ç‹¬çš„æŸ¥è¯¢åµŒå…¥å‡½æ•°"""
    embedder = get_text_embedder()
    try:
        return embedder._get_query_embedding(query)
    except Exception as e:
        print(f"âŒ æŸ¥è¯¢åµŒå…¥å¤±è´¥ï¼š{query[:50]}... é”™è¯¯ï¼š{e}")
        return [0.0] * 1536


def get_dimension(default_dim: int = 1536) -> int:
    """å›ºå®šè¿”å›DashScopeçš„1536ç»´"""
    return 1536


def _create_default_vector_store(dimension: int = 1536) -> QdrantClient:
    """åˆ›å»ºQdrantå®¢æˆ·ç«¯"""
    qdrant_url = os.getenv("QDRANT_URL")
    qdrant_api_key = os.getenv("QDRANT_API_KEY")
    if not qdrant_url or not qdrant_api_key:
        raise ValueError("âŒ Qdranté…ç½®ç¼ºå¤±ï¼")

    print(f"[RAG] å°è¯•è¿æ¥Qdrant: {qdrant_url}")

    try:
        # å…ˆæµ‹è¯•URLæ˜¯å¦å¯è®¿é—®
        parsed_url = urlparse(qdrant_url)
        hostname = parsed_url.netloc.split(':')[0]
        port = int(parsed_url.netloc.split(':')[1]) if ':' in parsed_url.netloc else 443

        print(f"[RAG] è§£æQdrantåœ°å€: {hostname}:{port}")

        # æµ‹è¯•DNSè§£æ
        try:
            addrinfo = socket.getaddrinfo(hostname, port, socket.AF_UNSPEC, socket.SOCK_STREAM)
            print(f"[RAG] DNSè§£ææˆåŠŸ: {addrinfo[0][4]}")
        except socket.gaierror as e:
            print(f"[ERROR] DNSè§£æå¤±è´¥: {e}")
            print("[INFO] è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒQDRANT_URLé…ç½®")
            raise

        # åˆ›å»ºQdrantå®¢æˆ·ç«¯
        client = QdrantClient(
            url=qdrant_url,
            api_key=qdrant_api_key,
            timeout=60  # åŠ é•¿è¶…æ—¶ï¼Œé€‚é…PDFå¤§æ–‡ä»¶
        )

        # æµ‹è¯•è¿æ¥
        print("[RAG] æµ‹è¯•Qdrantè¿æ¥...")
        client.get_collection("test_collection")
        print("[RAG] Qdrantè¿æ¥æˆåŠŸï¼")
        return client
    except Exception as e:
        print(f"[ERROR] Qdrantè¿æ¥å¤±è´¥: {e}")
        print("[INFO] æ£€æŸ¥QDRANT_URLå’ŒQDRANT_API_KEYé…ç½®æ˜¯å¦æ­£ç¡®")
        print("[INFO] æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        raise


def _preprocess_markdown_for_embedding(text: str) -> str:
    """é¢„å¤„ç†æ–‡æœ¬æå‡åµŒå…¥è´¨é‡"""
    # ç§»é™¤PDFé¡µç æ ‡è®°ã€å¤šä½™ç©ºæ ¼
    text = re.sub(r'=== ç¬¬\d+é¡µ ===', '', text)
    text = re.sub(r'\n{3,}', '\n\n', text)
    text = re.sub(r'\s{2,}', ' ', text)
    # ä¿ç•™ä¸­è‹±æ–‡å’ŒåŸºç¡€æ ‡ç‚¹
    text = re.sub(r'[^\w\s\u4e00-\u9fffï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''()ï¼ˆï¼‰ã€ã€‘]', '', text)
    return text.strip()


# ---------------------- è¾…åŠ©å‡½æ•°ï¼šæŸ¥è¯¢æ‰©å±• ----------------------
def _prompt_mqe(query: str, expansions: int = 2) -> List[str]:
    """å¤šæŸ¥è¯¢æ‰©å±•ï¼ˆMulti-Query Expansionï¼‰"""
    try:
        # åˆå§‹åŒ–LLMç”¨äºç”Ÿæˆæ‰©å±•æŸ¥è¯¢
        llm_config = {
            "api_key": os.getenv("LLM_API_KEY"),
            "model_id": os.getenv("LLM_MODEL_ID", "qwen-turbo"),
            "base_url": os.getenv("LLM_BASE_URL"),
            "timeout": 30
        }
        llm = HelloAgentsLLM(**{k: v for k, v in llm_config.items() if v})

        # MQEæç¤ºè¯
        messages = [{
            "role": "user",
            "content": f"""
            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŸ¥è¯¢æ‰©å±•åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹åŸå§‹æŸ¥è¯¢ï¼Œç”Ÿæˆ{expansions}ä¸ªä¸åŒçš„ã€è¯­ä¹‰ç›¸ä¼¼çš„æŸ¥è¯¢è¯­å¥ï¼Œç”¨äºå‘é‡æ•°æ®åº“æ£€ç´¢ã€‚
            è¦æ±‚ï¼š
            1. æ¯ä¸ªæŸ¥è¯¢ä¿æŒä¸åŸå§‹æŸ¥è¯¢çš„æ ¸å¿ƒè¯­ä¹‰ä¸€è‡´
            2. è¡¨è¾¾æ–¹å¼ä¸åŒï¼Œè¦†ç›–ä¸åŒçš„å…³é”®è¯å’Œå¥å¼
            3. ä»…è¿”å›æŸ¥è¯¢è¯­å¥ï¼Œæ¯è¡Œä¸€ä¸ªï¼Œä¸è¦æ·»åŠ é¢å¤–è¯´æ˜
            4. è¯­è¨€ä¸åŸå§‹æŸ¥è¯¢ä¿æŒä¸€è‡´

            åŸå§‹æŸ¥è¯¢ï¼š{query}
            """
        }]

        # è°ƒç”¨LLMç”Ÿæˆæ‰©å±•æŸ¥è¯¢
        response = llm.think(messages, temperature=0.5)
        if not response:
            raise Exception("LLMæœªè¿”å›æœ‰æ•ˆå“åº”")
        
        # å¤„ç†å“åº”ï¼Œç¡®ä¿å®ƒæ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²
        if isinstance(response, str):
            response_text = response.strip()
        else:
            # å¦‚æœæ˜¯ç”Ÿæˆå™¨ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            try:
                response_text = ''.join(response).strip()
            except:
                # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œè¿”å›åŸå§‹æŸ¥è¯¢
                raise Exception("æ— æ³•å¤„ç†LLMå“åº”")
        
        mqe_queries = [line.strip() for line in response_text.split("\n") if line.strip()]

        # ç¡®ä¿ç”Ÿæˆæ•°é‡ç¬¦åˆè¦æ±‚
        if len(mqe_queries) < expansions:
            mqe_queries += [query] * (expansions - len(mqe_queries))

        print(f"[MQE] ç”Ÿæˆ{len(mqe_queries)}ä¸ªæ‰©å±•æŸ¥è¯¢ï¼š{mqe_queries}")
        return mqe_queries[:expansions]
    except Exception as e:
        print(f"[WARNING] MQEæ‰©å±•å¤±è´¥ï¼š{e}ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢")
        return [query] * expansions


def _prompt_hyde(query: str) -> str:
    """HYDEï¼ˆHypothetical Document Embeddingsï¼‰"""
    try:
        # åˆå§‹åŒ–LLM
        llm_config = {
            "api_key": os.getenv("LLM_API_KEY"),
            "model_id": os.getenv("LLM_MODEL_ID", "qwen-turbo"),
            "base_url": os.getenv("LLM_BASE_URL"),
            "timeout": 30
        }
        llm = HelloAgentsLLM(**{k: v for k, v in llm_config.items() if v})

        # HYDEæç¤ºè¯
        messages = [{
            "role": "user",
            "content": f"""
            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£ç”ŸæˆåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æŸ¥è¯¢ï¼Œç”Ÿæˆä¸€æ®µå‡è®¾çš„ã€ç›¸å…³çš„æ–‡æ¡£å†…å®¹ï¼ˆçº¦100-200å­—ï¼‰ã€‚
            è¦æ±‚ï¼š
            1. å†…å®¹ä¸æŸ¥è¯¢é«˜åº¦ç›¸å…³ï¼Œç¬¦åˆè¯¥ä¸»é¢˜çš„çœŸå®æ–‡æ¡£ç‰¹å¾
            2. è¯­è¨€æµç•…ï¼Œç»“æ„åˆç†
            3. ä»…è¿”å›ç”Ÿæˆçš„æ–‡æ¡£å†…å®¹ï¼Œä¸è¦æ·»åŠ é¢å¤–è¯´æ˜

            æŸ¥è¯¢ï¼š{query}
            """
        }]

        # è°ƒç”¨LLMç”Ÿæˆå‡è®¾æ–‡æ¡£
        response = llm.think(messages, temperature=0.5)
        if not response:
            raise Exception("LLMæœªè¿”å›æœ‰æ•ˆå“åº”")
        
        # å¤„ç†å“åº”ï¼Œç¡®ä¿å®ƒæ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²
        if isinstance(response, str):
            hyde_text = response.strip()
        else:
            # å¦‚æœæ˜¯ç”Ÿæˆå™¨ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            try:
                hyde_text = ''.join(response).strip()
            except:
                # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
                raise Exception("æ— æ³•å¤„ç†LLMå“åº”")
        
        print(f"[HYDE] ç”Ÿæˆå‡è®¾æ–‡æ¡£ï¼š{hyde_text[:100]}...")
        return hyde_text
    except Exception as e:
        print(f"[WARNING] HYDEç”Ÿæˆå¤±è´¥ï¼š{e}")
        return ""


# ---------------------- æ ¸å¿ƒå‡½æ•°ï¼šå¢å¼ºç‰ˆå‘é‡æ£€ç´¢ ----------------------
def search_vectors_expanded(
        store=None,
        query: str = "",
        top_k: int = 8,
        rag_namespace: Optional[str] = None,
        only_rag_data: bool = True,
        score_threshold: Optional[float] = None,
        enable_mqe: bool = False,
        mqe_expansions: int = 2,
        enable_hyde: bool = False,
        candidate_pool_multiplier: int = 4,
) -> List[Dict]:
    """å¢å¼ºç‰ˆæ£€ç´¢ï¼šæ”¯æŒMQEå¤šæŸ¥è¯¢æ‰©å±•ã€HYDEå‡è®¾æ–‡æ¡£ã€å¤šå€™é€‰æ± èšåˆ"""
    if not query:
        return []

    # åˆ›å»ºé»˜è®¤å­˜å‚¨
    if store is None:
        store = _create_default_vector_store()

    # 1. æŸ¥è¯¢æ‰©å±•ï¼šåŸºç¡€æŸ¥è¯¢ + MQE + HYDE
    expansions: List[str] = [query]

    # MQEå¤šæŸ¥è¯¢æ‰©å±•
    if enable_mqe and mqe_expansions > 0:
        expansions.extend(_prompt_mqe(query, mqe_expansions))

    # HYDEå‡è®¾æ–‡æ¡£æ‰©å±•
    if enable_hyde:
        hyde_text = _prompt_hyde(query)
        if hyde_text:
            expansions.append(hyde_text)

    # å»é‡å’Œä¿®å‰ªæ‰©å±•æŸ¥è¯¢
    uniq: List[str] = []
    for e in expansions:
        if e and e not in uniq:
            uniq.append(e)
    expansions = uniq[: max(1, len(uniq))]
    print(f"[RAG] æœ€ç»ˆæ‰©å±•æŸ¥è¯¢åˆ—è¡¨ï¼š{expansions}")

    # 2. è®¡ç®—å€™é€‰æ± å¤§å°
    pool = max(top_k * candidate_pool_multiplier, 20)
    per = max(1, pool // max(1, len(expansions)))
    print(f"[RAG] å€™é€‰æ± å¤§å°ï¼š{pool}ï¼Œæ¯ä¸ªæŸ¥è¯¢å–{per}æ¡ç»“æœ")

    # 3. æ„å»ºè¿‡æ»¤å™¨
    filter_conditions = []
    if only_rag_data:
        filter_conditions.extend([
            FieldCondition(key="is_rag_data", match=MatchValue(value=True)),
            FieldCondition(key="data_source", match=MatchValue(value="rag_pipeline"))
        ])
    if rag_namespace:
        filter_conditions.append(
            FieldCondition(key="rag_namespace", match=MatchValue(value=rag_namespace))
        )

    qdrant_filter = Filter(must=filter_conditions) if filter_conditions else None

    # 4. æ”¶é›†æ‰€æœ‰æ‰©å±•æŸ¥è¯¢çš„æ£€ç´¢ç»“æœ
    agg: Dict[str, Dict] = {}
    for q in expansions:
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        qv = embed_query(q)

        # é€‚é…Qdrantæœ€æ–°ç‰ˆAPI
        try:
            # æ–°ç‰ˆAPIï¼šquery_points
            results = store.query_points(
                collection_name="test_collection",
                query=qv,
                limit=per,
                score_threshold=score_threshold,
                query_filter=qdrant_filter,
                with_payload=True,
                with_vectors=False
            )
            hits = results.points
        except AttributeError:
            # æ—§ç‰ˆAPIï¼šsearch
            results = store.search(
                collection_name="test_collection",
                query_vector=qv,
                limit=per,
                score_threshold=score_threshold,
                query_filter=qdrant_filter,
                with_payload=True
            )
            hits = results

        # è½¬æ¢ç»“æœæ ¼å¼å¹¶èšåˆ
        for h in hits:
            # æ„å»ºç»Ÿä¸€çš„ç»“æœæ ¼å¼
            hit_dict = {
                "id": h.id,
                "score": float(h.score),
                "metadata": h.payload,
                "content": h.payload.get("content") or h.payload.get("text")
            }
            mid = hit_dict["id"]

            # ä¿ç•™æœ€é«˜åˆ†çš„ç»“æœ
            if mid not in agg or hit_dict["score"] > agg[mid]["score"]:
                agg[mid] = hit_dict

    # 5. æŒ‰åˆ†æ•°æ’åºå¹¶è¿”å›top_k
    merged = list(agg.values())
    merged.sort(key=lambda x: x.get("score", 0.0), reverse=True)

    print(f"[RAG] èšåˆåå…±{len(merged)}æ¡ç»“æœï¼Œè¿”å›å‰{top_k}æ¡")
    return merged[:top_k]


# ---------------------- æ ¸å¿ƒæ–‡æ¡£å¤„ç†æ¨¡å— ----------------------
def _convert_to_markdown(path: str) -> str:
    """å¤šæ ¼å¼æ–‡æ¡£è½¬Markdownï¼ˆä¼˜å…ˆå¤„ç†PDFï¼‰"""
    if not os.path.exists(path):
        print(f"[ERROR] æ–‡ä»¶ä¸å­˜åœ¨: {path}")
        return ""

    ext = (os.path.splitext(path)[1] or '').lower()
    if ext == '.pdf':
        return _enhanced_pdf_processing(path)

    # å…¶ä»–æ ¼å¼é€šç”¨è½¬æ¢
    md_instance = _get_markitdown_instance()
    if md_instance is None:
        return _fallback_text_reader(path)

    try:
        result = md_instance.convert(path)
        markdown_text = getattr(result, "text_content", None)
        if isinstance(markdown_text, str) and markdown_text.strip():
            print(f"[RAG] è½¬æ¢æˆåŠŸ: {path} -> {len(markdown_text)} å­—ç¬¦")
            return markdown_text
        return ""
    except Exception as e:
        print(f"[WARNING] è½¬æ¢å¤±è´¥ {path}: {e}")
        return _fallback_text_reader(path)


def _split_paragraphs_with_headings(text: str) -> List[Dict]:
    """æŒ‰æ ‡é¢˜/æ®µè½åˆ†å‰²æ–‡æœ¬ï¼ˆä¼˜åŒ–ç‰ˆï¼šæ·»åŠ è¿›åº¦+æ€§èƒ½æå‡ï¼‰"""
    lines = text.splitlines()
    total_lines = len(lines)
    print(f"[RAG] å¼€å§‹åˆ†å‰²æ–‡æœ¬ï¼šå…±{total_lines}è¡Œ")

    heading_stack: List[str] = []
    paragraphs: List[Dict] = []
    buf: List[str] = []
    char_pos = 0

    def flush_buf(end_pos: int):
        if not buf:
            return
        content = "\n".join(buf).strip()
        if len(content) < 10:  # è¿‡æ»¤è¿‡çŸ­çš„æ— æ•ˆæ®µè½
            return
        paragraphs.append({
            "content": content,
            "heading_path": " > ".join(heading_stack) if heading_stack else None,
            "start": max(0, end_pos - len(content)),
            "end": end_pos,
        })

    # é€è¡Œå¤„ç†å¹¶æ‰“å°è¿›åº¦
    for idx, ln in enumerate(lines):
        # æ¯å¤„ç†100è¡Œæ‰“å°ä¸€æ¬¡è¿›åº¦
        if idx % 100 == 0 and idx > 0:
            print(f"[RAG] æ–‡æœ¬åˆ†å‰²è¿›åº¦ï¼š{idx}/{total_lines}è¡Œ")

        raw = ln.strip()
        if not raw:
            flush_buf(char_pos)
            buf = []
        elif raw.startswith("#"):
            # å¤„ç†Markdownæ ‡é¢˜
            flush_buf(char_pos)
            level = len(raw) - len(raw.lstrip('#'))
            title = raw.lstrip('#').strip()
            if level <= 0:
                level = 1
            if level <= len(heading_stack):
                heading_stack = heading_stack[:level - 1]
            heading_stack.append(title)
        else:
            # æ™®é€šæ®µè½å†…å®¹
            buf.append(raw)
        char_pos += len(ln) + 1

    flush_buf(char_pos)
    print(f"[RAG] æ–‡æœ¬åˆ†å‰²å®Œæˆï¼šå…±æå–{len(paragraphs)}ä¸ªæ®µè½")

    # å…œåº•ï¼šå¦‚æœæ²¡æœ‰åˆ†å‰²å‡ºæ®µè½ï¼Œæ•´æ®µä½œä¸ºä¸€ä¸ª
    if not paragraphs:
        clean_text = text.strip()
        if clean_text:
            paragraphs = [{"content": clean_text, "heading_path": None, "start": 0, "end": len(clean_text)}]
            print(f"[RAG] å…œåº•å¤„ç†ï¼šæ•´æ®µæ–‡æœ¬ä½œä¸º1ä¸ªæ®µè½")

    return paragraphs


def _chunk_paragraphs(paragraphs: List[Dict], chunk_tokens: int = 512, overlap_tokens: int = 50) -> List[Dict]:
    """æ™ºèƒ½åˆ†å—ï¼ˆä¼˜åŒ–ç‰ˆï¼šæ·»åŠ è¿›åº¦+æå‰è®¡ç®—Tokenï¼‰"""
    total_paragraphs = len(paragraphs)
    print(f"[RAG] å¼€å§‹æ™ºèƒ½åˆ†å—ï¼šå…±{total_paragraphs}ä¸ªæ®µè½ï¼Œç›®æ ‡å—å¤§å°{chunk_tokens}Tokenï¼Œé‡å {overlap_tokens}Token")

    # æå‰è®¡ç®—æ‰€æœ‰æ®µè½çš„Tokenæ•°ï¼Œé¿å…é‡å¤è®¡ç®—
    paragraph_tokens = []
    for idx, p in enumerate(paragraphs):
        token_len = _approx_token_len(p["content"]) or 1
        paragraph_tokens.append(token_len)
        if idx % 50 == 0 and idx > 0:
            print(f"[RAG] Tokenè®¡ç®—è¿›åº¦ï¼š{idx}/{total_paragraphs}æ®µè½")

    chunks: List[Dict] = []
    cur: List[Dict] = []
    cur_tokens = 0
    i = 0

    while i < total_paragraphs:
        p = paragraphs[i]
        p_tokens = paragraph_tokens[i]

        if cur_tokens + p_tokens <= chunk_tokens or not cur:
            cur.append(p)
            cur_tokens += p_tokens
            i += 1
        else:
            # ç”Ÿæˆåˆ†å—
            content = "\n\n".join(x["content"] for x in cur)
            chunks.append({
                "content": content,
                "start": cur[0]["start"],
                "end": cur[-1]["end"],
                "heading_path": next((x["heading_path"] for x in reversed(cur) if x.get("heading_path")), None),
            })
            # ä¿ç•™é‡å éƒ¨åˆ†
            if overlap_tokens > 0 and cur:
                kept: List[Dict] = []
                kept_tokens = 0
                for x in reversed(cur):
                    t = _approx_token_len(x["content"]) or 1
                    if kept_tokens + t > overlap_tokens:
                        break
                    kept.append(x)
                    kept_tokens += t
                cur = list(reversed(kept))
                cur_tokens = kept_tokens
            else:
                cur = []
                cur_tokens = 0

        # æ‰“å°åˆ†å—è¿›åº¦
        if len(chunks) % 10 == 0 and len(chunks) > 0:
            print(f"[RAG] åˆ†å—è¿›åº¦ï¼šå·²ç”Ÿæˆ{len(chunks)}ä¸ªå—ï¼Œå¤„ç†{i}/{total_paragraphs}æ®µè½")

    # å¤„ç†æœ€åä¸€ä¸ªåˆ†å—
    if cur:
        content = "\n\n".join(x["content"] for x in cur)
        chunks.append({
            "content": content,
            "start": cur[0]["start"],
            "end": cur[-1]["end"],
            "heading_path": next((x["heading_path"] for x in reversed(cur) if x.get("heading_path")), None),
        })

    print(f"[RAG] åˆ†å—å®Œæˆï¼šå…±ç”Ÿæˆ{len(chunks)}ä¸ªæ–‡æœ¬å—")
    return chunks


def _approx_token_len(text: str) -> int:
    """è¿‘ä¼¼Tokené•¿åº¦ï¼ˆä¸­è‹±æ–‡æ··åˆï¼‰"""
    cjk = sum(1 for ch in text if _is_cjk(ch))
    non_cjk_tokens = len([t for t in text.split() if t])
    return cjk + non_cjk_tokens


def _is_cjk(ch: str) -> bool:
    """åˆ¤æ–­CJKå­—ç¬¦"""
    code = ord(ch)
    return (
            0x4E00 <= code <= 0x9FFF or 0x3400 <= code <= 0x4DBF or
            0x20000 <= code <= 0x2A6DF or 0x2A700 <= code <= 0x2B73F or
            0x2B740 <= code <= 0x2B81F or 0x2B820 <= code <= 0x2CEAF or
            0xF900 <= code <= 0xFAFF
    )


def index_chunks(
        store=None,
        chunks: List[Dict] = None,
        cache_db: Optional[str] = None,
        batch_size: int = 32,
        rag_namespace: str = "default",
        collection_name: str = "test_collection"
) -> None:
    """æ‰¹é‡å‘é‡å…¥åº“ï¼ˆä¼˜åŒ–ç‰ˆï¼šæ·»åŠ è¯¦ç»†è¿›åº¦+è·³è¿‡é‡å¤ç´¢å¼•+è¶…æ—¶å¤„ç†ï¼‰"""
    if not chunks:
        print("[RAG] æ— åˆ†å—æ•°æ®å¯å…¥åº“")
        return

    # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
    embedder = get_text_embedder()
    dimension = get_dimension(1536)

    # åˆ›å»ºQdrantå®¢æˆ·ç«¯
    if store is None:
        store = _create_default_vector_store(dimension)
        print(f"[RAG] åˆå§‹åŒ–Qdrantå­˜å‚¨ï¼Œç»´åº¦ï¼š{dimension}")

    # é¢„å¤„ç†æ–‡æœ¬
    processed_texts = []
    total_chunks = len(chunks)
    print(f"[RAG] å¼€å§‹é¢„å¤„ç†æ–‡æœ¬ï¼šå…±{total_chunks}ä¸ªåˆ†å—")

    for idx, c in enumerate(chunks):
        processed_content = _preprocess_markdown_for_embedding(c["content"])
        if processed_content and len(processed_content) > 10:  # è¿‡æ»¤è¿‡çŸ­æ–‡æœ¬
            processed_texts.append(processed_content)
        else:
            processed_texts.append("æ— æ•ˆæ–‡æœ¬")  # é¿å…ç©ºæ–‡æœ¬åµŒå…¥

        # æ‰“å°é¢„å¤„ç†è¿›åº¦
        if idx % 20 == 0 and idx > 0:
            print(f"[RAG] æ–‡æœ¬é¢„å¤„ç†è¿›åº¦ï¼š{idx}/{total_chunks}åˆ†å—")

    print(f"[RAG] å¼€å§‹åµŒå…¥ï¼šå…±{len(processed_texts)}ä¸ªåˆ†å—ï¼Œæ‰¹æ¬¡å¤§å°{batch_size}")

    # æ‰¹é‡ç”Ÿæˆå‘é‡
    vecs: List[List[float]] = []
    total_batches = (len(processed_texts) + batch_size - 1) // batch_size

    for batch_idx in range(total_batches):
        start_idx = batch_idx * batch_size
        end_idx = min((batch_idx + 1) * batch_size, len(processed_texts))
        part = processed_texts[start_idx:end_idx]

        print(f"[RAG] å¤„ç†åµŒå…¥æ‰¹æ¬¡ {batch_idx + 1}/{total_batches}ï¼šåˆ†å—{start_idx + 1}-{end_idx}")

        try:
            # ä½¿ç”¨ä¿®å¤åçš„æ‰¹é‡åµŒå…¥æ–¹æ³•
            part_vecs = embedder._get_text_embeddings(part)
            vecs.extend(part_vecs)
            print(f"[RAG] æ‰¹æ¬¡{batch_idx + 1}åµŒå…¥æˆåŠŸï¼š{len(part_vecs)}ä¸ªå‘é‡")
        except Exception as e:
            print(f"[WARNING] æ‰¹æ¬¡{batch_idx + 1}åµŒå…¥å¤±è´¥ï¼š{e}ï¼Œä¸ºæ¯ä¸ªæ–‡æœ¬å¡«å……é›¶å‘é‡")
            vecs.extend([[0.0] * dimension for _ in part])

    # æ‰¹é‡æ’å…¥Qdrant
    points = []
    valid_count = 0
    print(f"[RAG] å¼€å§‹å‡†å¤‡Qdrantå…¥åº“æ•°æ®ï¼šå…±{len(chunks)}ä¸ªåˆ†å—")

    for idx, (chunk, vec) in enumerate(zip(chunks, vecs)):
        # è¿‡æ»¤å…¨é›¶å‘é‡
        if all(v == 0.0 for v in vec):
            if idx % 50 == 0:
                print(f"[RAG] è¿‡æ»¤è¿›åº¦ï¼š{idx}/{len(chunks)}åˆ†å—ï¼ˆå·²è¿‡æ»¤{valid_count}ä¸ªæœ‰æ•ˆå‘é‡ï¼‰")
            continue

        # ç”Ÿæˆå”¯ä¸€UUID
        chunk_id = str(uuid.uuid5(uuid.NAMESPACE_OID, f"{rag_namespace}_{collection_name}_pdf_{idx}"))
        point = PointStruct(
            id=chunk_id,
            vector=vec,
            payload={
                "content": chunk["content"],
                "heading_path": chunk.get("heading_path"),
                "start": chunk["start"],
                "end": chunk["end"],
                "namespace": rag_namespace,
                "rag_namespace": rag_namespace,
                "source": "test.pdf",
                "is_rag_data": True,
                "data_source": "rag_pipeline",
                "memory_type": "rag_chunk",
                "create_time": time.time()
            }
        )
        points.append(point)
        valid_count += 1

        # æ‰“å°å…¥åº“å‡†å¤‡è¿›åº¦
        if idx % 50 == 0 and idx > 0:
            print(f"[RAG] å…¥åº“æ•°æ®å‡†å¤‡è¿›åº¦ï¼š{idx}/{len(chunks)}åˆ†å—ï¼ˆæœ‰æ•ˆå‘é‡{valid_count}ä¸ªï¼‰")

    # åˆ†æ‰¹æ¬¡æ’å…¥
    if points:
        total_insert_batches = (len(points) + 99) // 100
        print(f"[RAG] å¼€å§‹æ’å…¥Qdrantï¼šå…±{len(points)}ä¸ªå‘é‡ï¼Œåˆ†{total_insert_batches}æ‰¹æ¬¡")

        for batch_idx in range(total_insert_batches):
            start_idx = batch_idx * 100
            end_idx = min((batch_idx + 1) * 100, len(points))
            batch_points = points[start_idx:end_idx]

            print(f"[RAG] æ’å…¥æ‰¹æ¬¡ {batch_idx + 1}/{total_insert_batches}ï¼š{len(batch_points)}ä¸ªå‘é‡")

            # è®¾ç½®è¶…æ—¶æ—¶é—´ï¼Œé¿å…å¡ä½
            try:
                store.upsert(
                    collection_name=collection_name,
                    points=batch_points,
                    wait=True
                )
                print(f"[RAG] æ‰¹æ¬¡{batch_idx + 1}æ’å…¥æˆåŠŸ")
            except Exception as e:
                print(f"[WARNING] æ‰¹æ¬¡{batch_idx + 1}æ’å…¥å¤±è´¥ï¼š{e}")
                continue

        print(f"âœ… PDFå…¥åº“å®Œæˆï¼šå…±{valid_count}ä¸ªæœ‰æ•ˆå‘é‡ï¼Œé›†åˆï¼š{collection_name}")
    else:
        print("âŒ æ— æœ‰æ•ˆå‘é‡å¯å…¥åº“ï¼Œè¯·æ£€æŸ¥DashScopeåµŒå…¥ç»“æœ")


# ---------------------- Qdrant IDç”Ÿæˆå·¥å…· ----------------------
def generate_valid_qdrant_id(document_id: str) -> str:
    return str(uuid.uuid5(uuid.NAMESPACE_OID, document_id))


# ---------------------- å¢å¼ºç‰ˆRAGTool ----------------------
class EnhancedQdrantRAGTool(RAGTool):
    def __init__(self, knowledge_base_path, collection_name, rag_namespace, qdrant_client=None, embedding_model=None):
        super().__init__(
            knowledge_base_path=knowledge_base_path,
            collection_name=collection_name,
            rag_namespace=rag_namespace,
            expandable=False
        )
        self.extra_qdrant_client = qdrant_client
        self.extra_embedding_model = embedding_model

    def execute(self, action, **kwargs):
        """æ‰§è¡Œå·¥å…·æ“ä½œ"""
        return self.run({"action": action, **kwargs})

    def get_name(self):
        """è·å–å·¥å…·åç§°"""
        return "rag"

    def get_description(self):
        """è·å–å·¥å…·æè¿°"""
        return "å¢å¼ºç‰ˆRAGå·¥å…·ï¼Œæ”¯æŒPDFå¯¼å…¥ã€æ–‡æœ¬æ·»åŠ å’Œæ™ºèƒ½æ£€ç´¢"

    def get_parameters(self):
        """è·å–å·¥å…·å‚æ•°"""
        return super().get_parameters()


# ---------------------- åˆå§‹åŒ–æœåŠ¡å‡½æ•° ----------------------
def init_services():
    """åˆå§‹åŒ–Qdrantå’ŒDashScope"""
    # DashScopeåµŒå…¥æ¨¡å‹
    dashscope_api_key = os.getenv("DASHSCOPE_API_KEY")
    if not dashscope_api_key:
        raise ValueError("âŒ è¯·é…ç½®DASHSCOPE_API_KEYåˆ°.envæ–‡ä»¶")

    # ä½¿ç”¨ä¿®å¤åçš„å®‰å…¨åµŒå…¥æ¨¡å‹
    embedding = SafeDashScopeEmbedding(
        model_name="text-embedding-v1",
        api_key=dashscope_api_key
    )
    print("âœ… DashScopeåµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")

    # Qdrantå®¢æˆ·ç«¯
    qdrant_url = os.getenv("QDRANT_URL")
    qdrant_api_key = os.getenv("QDRANT_API_KEY")
    if not qdrant_url or not qdrant_api_key:
        raise ValueError("âŒ è¯·é…ç½®QDRANT_URLå’ŒQDRANT_API_KEYåˆ°.envæ–‡ä»¶")

    print(f"[RAG] å°è¯•åˆå§‹åŒ–Qdrantå®¢æˆ·ç«¯: {qdrant_url}")

    # å…ˆæµ‹è¯•DNSè§£æ
    try:
        parsed_url = urlparse(qdrant_url)
        hostname = parsed_url.netloc.split(':')[0]
        port = int(parsed_url.netloc.split(':')[1]) if ':' in parsed_url.netloc else 443

        print(f"[RAG] è§£æQdrantåœ°å€: {hostname}:{port}")

        # æµ‹è¯•DNSè§£æ
        addrinfo = socket.getaddrinfo(hostname, port, socket.AF_UNSPEC, socket.SOCK_STREAM)
        print(f"[RAG] DNSè§£ææˆåŠŸ: {addrinfo[0][4]}")

    except socket.gaierror as e:
        print(f"[ERROR] DNSè§£æå¤±è´¥: {e}")
        print("[INFO] è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒQDRANT_URLé…ç½®")
        print("[INFO] å°è¯•ä½¿ç”¨IPåœ°å€æ›¿ä»£åŸŸå")

        # å°è¯•ä½¿ç”¨ç¡¬ç¼–ç çš„IPåœ°å€
        print("[INFO] å°è¯•ä½¿ç”¨å¤‡ç”¨IPåœ°å€...")
        backup_ip = "34.248.146.137"  # ç¤ºä¾‹IPï¼Œå®é™…éœ€æ›¿æ¢ä¸ºä½ çš„è§£æç»“æœ
        backup_url = f"{parsed_url.scheme}://{backup_ip}:{port}{parsed_url.path}"
        print(f"[INFO] ä½¿ç”¨å¤‡ç”¨URL: {backup_url}")
        qdrant_url = backup_url

        # å†æ¬¡æµ‹è¯•è¿æ¥
        try:
            print("[RAG] æµ‹è¯•å¤‡ç”¨URLè¿æ¥...")
            test_client = QdrantClient(
                url=qdrant_url,
                api_key=qdrant_api_key,
                timeout=60
            )
            test_client.get_collections()
            print("[RAG] å¤‡ç”¨URLè¿æ¥æˆåŠŸï¼")
        except Exception as backup_e:
            print(f"[ERROR] å¤‡ç”¨URLè¿æ¥å¤±è´¥: {backup_e}")
            raise

    except Exception as e:
        print(f"[ERROR] Qdrantåˆå§‹åŒ–å‰ç½®æ£€æŸ¥å¤±è´¥: {e}")
        raise

    # åˆ›å»ºQdrantå®¢æˆ·ç«¯
    try:
        client = QdrantClient(
            url=qdrant_url,
            api_key=qdrant_api_key,
            timeout=60
        )

        # æµ‹è¯•è¿æ¥
        print("[RAG] æµ‹è¯•Qdrantè¿æ¥...")
        client.get_collections()
        print("[RAG] Qdrantè¿æ¥æˆåŠŸï¼")

    except Exception as e:
        print(f"[ERROR] Qdrantå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        raise

    # åˆ›å»ºé›†åˆï¼ˆç¡®ä¿å­˜åœ¨ï¼‰
    collection_name = "test_collection"
    try:
        if not client.collection_exists(collection_name):
            client.create_collection(
                collection_name=collection_name,
                vectors_config=VectorParams(size=1536, distance="Cosine")
            )
            print(f"âœ… åˆ›å»ºQdranté›†åˆï¼š{collection_name}")
        else:
            print(f"âœ… Qdranté›†åˆ {collection_name} å·²å­˜åœ¨")
    except Exception as e:
        print(f"[ERROR] é›†åˆæ“ä½œå¤±è´¥: {e}")
        raise

    return client, embedding


# ---------------------- ä¸»ç¨‹åº ----------------------
if __name__ == "__main__":
    try:
        # 1. åˆå§‹åŒ–åŸºç¡€æœåŠ¡
        print("[RAG] åˆå§‹åŒ–æ ¸å¿ƒæœåŠ¡...")
        qdrant_client, embedding_model = init_services()

        # 2. åˆ›å»ºLLMå’ŒAgent
        print("[RAG] åˆ›å»ºLLMå’ŒAgent...")
        llm_config = {
            "api_key": os.getenv("LLM_API_KEY"),
            "model_id": os.getenv("LLM_MODEL_ID", "qwen-turbo"),
            "base_url": os.getenv("LLM_BASE_URL"),
            "timeout": 60
        }
        llm = HelloAgentsLLM(**{k: v for k, v in llm_config.items() if v})
        agent = SimpleAgent(name="PDFçŸ¥è¯†åŠ©æ‰‹", llm=llm)

        # 3. æ³¨å†Œå¢å¼ºç‰ˆRAGå·¥å…·
        print("[RAG] æ³¨å†Œå¢å¼ºç‰ˆRAGå·¥å…·...")
        rag_tool = EnhancedQdrantRAGTool(
            knowledge_base_path="./knowledge_base",
            collection_name="test_collection",
            rag_namespace="test",
            qdrant_client=qdrant_client,
            embedding_model=embedding_model
        )
        tool_registry = ToolRegistry()
        tool_registry.register_tool(rag_tool)
        agent.tool_registry = tool_registry
        print("âœ… å¢å¼ºç‰ˆRAGå·¥å…·æ³¨å†ŒæˆåŠŸ")

        # 4. å¯¼å…¥test.pdfï¼ˆä¼˜åŒ–ç‰ˆï¼šæ·»åŠ å®Œæ•´è¿›åº¦åé¦ˆï¼‰
        print("\n===== å¯¼å…¥test.pdf =====")
        if os.path.exists("./test.pdf"):
            start_time = time.time()
            print(f"[RAG] å¼€å§‹å¤„ç†PDFï¼š{time.strftime('%Y-%m-%d %H:%M:%S')}")

            # æ‰‹åŠ¨è°ƒç”¨è§£æå’Œå…¥åº“æµç¨‹
            pdf_text = _convert_to_markdown("./test.pdf")

            if pdf_text:
                # æ–‡æœ¬åˆ†å‰²
                paragraphs = _split_paragraphs_with_headings(pdf_text)

                # æ™ºèƒ½åˆ†å—
                chunks = _chunk_paragraphs(paragraphs)
                print(f"[RAG] PDFåˆ†å‰²å‡º {len(chunks)} ä¸ªæœ‰æ•ˆåˆ†å—")

                # å‘é‡å…¥åº“
                index_chunks(
                    store=qdrant_client,
                    chunks=chunks,
                    rag_namespace="test",
                    collection_name="test_collection"
                )

                # è®¡ç®—æ€»è€—æ—¶
                total_time = time.time() - start_time
                print(f"[RAG] PDFå¤„ç†å®Œæˆï¼æ€»è€—æ—¶ï¼š{total_time:.2f}ç§’")
            else:
                print("âŒ PDFè§£æå¤±è´¥ï¼Œæ— æœ‰æ•ˆæ–‡æœ¬")
        else:
            print("âš ï¸ test.pdf æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡å¯¼å…¥æ“ä½œ")

        # 5. åŸºç¡€æ£€ç´¢æµ‹è¯•
        print("\n===== åŸºç¡€æ£€ç´¢æµ‹è¯• ======")
        search_result = search_vectors_expanded(
            store=qdrant_client,
            query="å”‡è¯» æ·±åº¦å­¦ä¹  è®ºæ–‡",
            rag_namespace="test",
            top_k=3,
            enable_mqe=False  # å…ˆå…³é—­MQEï¼Œç®€åŒ–æµ‹è¯•
        )
        if search_result:
            for idx, res in enumerate(search_result, 1):
                print(f"\nğŸ“ æ£€ç´¢ç»“æœ{idx}ï¼ˆç›¸ä¼¼åº¦ï¼š{res['score']:.4f}ï¼‰:")
                print(f"   å†…å®¹ï¼š{res['content'][:200]}...")
        else:
            print("âŒ æœªæ£€ç´¢åˆ°ç›¸å…³å†…å®¹")

    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­ç¨‹åºæ‰§è¡Œ")
    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œå¤±è´¥ï¼š{str(e)[:500]}")
        import traceback

        traceback.print_exc()
    finally:
        print("\nğŸ‰ ç¨‹åºæ‰§è¡Œå®Œæˆ")
